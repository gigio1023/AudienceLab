---
description: Crawl Instagram user activity data using BFS traversal from a seed user.
---

# Instagram BFS Crawler

Crawl Instagram user activity data using BFS traversal from a seed user. Supports incremental crawling to avoid re-crawling existing data.

## Arguments

- `$ARGUMENTS`: Seed username and options in format: `<seed_username> [--max-users <N>] [--output <path>] [--username <ig_username>] [--password <ig_password>] [--max-comments <N>] [--max-likers <N>] [--full]`

## Default Values

- `max_users`: 10 (maximum number of users to crawl including seed)
- `output`: `./instagram_crawl_results/` (output directory)
- `depth`: 1 (fixed - seed user + their followers)
- `mode`: incremental (default) - only crawl new data; use `--full` flag for full re-crawl
- `max_comments`: 0 (unlimited - extract ALL available comments per post)
- `max_likers`: 0 (unlimited - extract ALL available likers per post)

## Workflow

### Step 1: Parse Arguments

Parse the input arguments:
- Extract seed username (required)
- Extract max users limit (default: 10)
- Extract output path (default: ./instagram_crawl_results/)
- Extract Instagram username (optional, for login)
- Extract Instagram password (optional, for login)
- Extract max comments per post (default: 0 = unlimited)
- Extract max likers per post (default: 0 = unlimited)
- Check for `--full` flag (if present, do full crawl; otherwise incremental)

Example: `/crawl_instagram johndoe --max-users 5 --output ./data/ --username myuser@email.com --password mypassword --max-comments 50 --max-likers 100`

### Step 2: Check Existing Data (Incremental Mode)

**IMPORTANT: This step enables incremental crawling.**

Before crawling, check for existing data:

1. Check if `{output}/crawl_summary.json` exists
2. Check if `{output}/{seed_username}/data.json` exists
3. If exists, read and parse the existing data:
   ```javascript
   // Load existing data
   existingData = JSON.parse(read("{output}/{seed_username}/data.json"))
   existingPostUrls = new Set(existingData.posts.map(p => p.url))
   lastCrawlTime = existingData.crawled_at
   ```

4. Store existing data for later comparison:
   - `existingPosts`: Array of already crawled post URLs
   - `existingFollowers`: Array of already crawled follower usernames
   - `lastCrawlTime`: Timestamp of last crawl

5. If `--full` flag is provided, skip this step and do full crawl

**Display to user:**
```
Found existing data for {username}:
- Last crawled: {lastCrawlTime}
- Existing posts: {count}
- Mode: Incremental (will only crawl new posts)
```

### Step 3: Initialize Output Directory

Create output directory using Bash:
```bash
mkdir -p {output}/{seed_username}
```

### Step 4: Instagram Login (if credentials provided)

If `--username` and `--password` are provided, perform login using Playwright MCP:

1. Navigate to login page:
   - Use `browser_navigate` to open `https://www.instagram.com/accounts/login/`

2. Wait for page load and dismiss cookie consent if present:
   - Use `browser_snapshot` to get page state
   - If cookie dialog exists, use `browser_click` on close/accept button

3. Fill login form:
   - Use `browser_snapshot` to identify form elements
   - Use `browser_type` for username field (look for input with "phone", "username", or "email" placeholder)
   - Use `browser_type` for password field (look for input with "password" placeholder)

4. Submit login:
   - Use `browser_click` on the login button (ë¡œê·¸ì¸/Log in)
   - Use `browser_wait` for navigation to complete

5. Verify login success:
   - Use `browser_snapshot` to check for navigation elements (Home, Search, Explore)
   - If login fails or 2FA required, report to user

### Step 5: Crawl Seed User (Incremental)

Navigate to seed user's profile and extract data:

1. Navigate to profile:
   - Use `browser_navigate` to open `https://www.instagram.com/{seed_username}/`

2. Extract profile info (always update):
   - Use `browser_snapshot` to get page content
   - Parse: display name, bio, follower count, following count, post count, verification status
   - **Profile info is always updated** even in incremental mode

3. **Incremental Post Extraction:**
   - Use `browser_snapshot` to find post thumbnails
   - For each post visible on the grid:
     a. Extract the post URL from the link
     b. **Check if URL exists in `existingPostUrls`**
     c. **If post already exists, SKIP it** (don't click or extract details)
     d. **If post is NEW, extract ALL details:**
        - Use `browser_click` to open post modal
        - Use `browser_snapshot` to extract caption, like count, comment count, timestamp
        - **CRITICAL: Extract comments:**
          * Look for "View all X comments" or "ëŒ“ê¸€ Xê°œ ëª¨ë‘ ë³´ê¸°" link and click if present
          * Use `browser_scroll` in comments area to load more
          * For each comment: extract username, text, timestamp, like_count
          * **Keep scrolling and extracting until:**
            - No more comments load (reached end), OR
            - Reached `max_comments` limit (if > 0)
          * If `max_comments` is 0 (unlimited), extract ALL available comments
        - **CRITICAL: Extract likers:**
          * Click on the likes count text (e.g., "1,234 likes", "ì¢‹ì•„ìš” Xê°œ")
          * Use `browser_snapshot` to get likers modal
          * For each liker: extract username, display_name
          * Use `browser_scroll` to load more likers
          * **Keep scrolling and extracting until:**
            - No more likers load (reached end), OR
            - Reached `max_likers` limit (if > 0)
          * If `max_likers` is 0 (unlimited), extract ALL available likers
          * Press "Escape" to close likers modal
        - Use `browser_press_key` with "Escape" to close post modal
        - Use `browser_wait_for` with time: 2 seconds between posts
        - Add to `newPosts` array with ALL extracted data

4. **Merge existing and new data:**
   ```javascript
   // IMPORTANT: Each new post MUST have these fields:
   // - url, post_id, type, caption, like_count, comment_count
   // - timestamp, crawled_at
   // - comments: [{username, text, timestamp?, like_count?}]
   // - likers: [{username, display_name}]

   // Validate new posts have comments and likers arrays
   for (post of newPosts) {
     if (!post.comments) post.comments = []
     if (!post.likers) post.likers = []
     if (!post.like_count) post.like_count = 0
     if (!post.comment_count) post.comment_count = 0
   }

   // Merge posts: new posts first, then existing posts
   mergedPosts = [...newPosts, ...existingData.posts]

   // Update crawl timestamp
   userData.crawled_at = new Date().toISOString()
   userData.last_incremental_crawl = new Date().toISOString()
   userData.posts = mergedPosts
   ```

5. Save merged data to `{output}/{seed_username}/data.json`

**Display progress:**
```
Crawling {username}:
- Skipped {skipped_count} existing posts
- Found {new_count} new posts
- Total posts now: {total_count}
```

### Step 6: Collect Follower Usernames (Incremental)

From the seed user's profile:

1. Open followers modal:
   - Use `browser_snapshot` to find followers link
   - Use `browser_click` on followers count

2. Scroll and collect usernames:
   - Use `browser_snapshot` to get visible followers
   - **Filter out already crawled followers** from `existingFollowers`
   - Use `browser_scroll` to load more followers
   - Repeat until `max_users - 1` NEW followers collected (or no more available)

3. Close modal:
   - Use `browser_press_key` with "Escape"

**Logic for follower selection:**
```javascript
// Load existing crawled followers from crawl_summary.json
existingCrawledUsers = crawlSummary.users.map(u => u.username)

// Collect new followers only
newFollowersToProcess = []
for (follower of visibleFollowers) {
  if (!existingCrawledUsers.includes(follower.username)) {
    newFollowersToProcess.push(follower)
  }
  if (newFollowersToProcess.length >= maxUsers - 1 - existingCrawledUsers.length) {
    break
  }
}
```

### Step 7: Crawl Followers (Incremental)

For each follower to crawl:

1. **Check if follower data already exists:**
   ```javascript
   followerDataPath = "{output}/{follower_username}/data.json"
   if (fileExists(followerDataPath)) {
     existingFollowerData = JSON.parse(read(followerDataPath))
     // Apply same incremental logic as seed user
   }
   ```

2. Navigate to follower's Instagram profile using `browser_navigate`
3. Extract profile data (always update)
4. **Incremental post extraction** (same logic as Step 5)
5. Merge and save to `{output}/{follower_username}/data.json`

### Step 8: Aggregate Results (Merge with Existing Summary)

After all crawls complete, update the summary:

```json
// {output}/crawl_summary.json
{
  "seed_user": "johndoe",
  "first_crawl_timestamp": "2024-01-20T10:30:00Z",
  "last_crawl_timestamp": "2024-01-21T15:45:00Z",
  "crawl_history": [
    {
      "timestamp": "2024-01-20T10:30:00Z",
      "mode": "full",
      "users_crawled": 5,
      "new_posts_found": 47
    },
    {
      "timestamp": "2024-01-21T15:45:00Z",
      "mode": "incremental",
      "users_crawled": 5,
      "new_posts_found": 12
    }
  ],
  "total_users_crawled": 5,
  "max_users_limit": 10,
  "depth": 1,
  "users": [
    {
      "username": "johndoe",
      "role": "seed",
      "status": "success",
      "data_file": "./johndoe/data.json",
      "last_crawled": "2024-01-21T15:45:00Z",
      "total_posts_collected": 52
    },
    {
      "username": "follower1",
      "role": "follower",
      "status": "success",
      "data_file": "./follower1/data.json",
      "last_crawled": "2024-01-21T15:45:00Z",
      "total_posts_collected": 28
    }
  ],
  "errors": []
}
```

## Incremental Crawling Algorithm

```
FUNCTION incrementalCrawl(username, outputPath):
    existingData = loadExistingData(outputPath, username)
    existingPostUrls = extractPostUrls(existingData)

    navigateToProfile(username)
    profileInfo = extractProfileInfo()  // Always update

    newPosts = []
    visiblePosts = getVisiblePostsFromGrid()

    FOR EACH post IN visiblePosts:
        IF post.url NOT IN existingPostUrls:
            postDetails = openAndExtractPost(post)
            newPosts.append(postDetails)
            PRINT "New post found: {post.url}"
        ELSE:
            PRINT "Skipping existing post: {post.url}"

    // Merge: new posts at the beginning (most recent)
    mergedPosts = newPosts + existingData.posts

    // Remove duplicates (by URL)
    mergedPosts = deduplicateByUrl(mergedPosts)

    saveData(username, profileInfo, mergedPosts)

    RETURN {
        newPostsCount: newPosts.length,
        skippedCount: visiblePosts.length - newPosts.length,
        totalPosts: mergedPosts.length
    }
```

## Playwright MCP Tools Reference

| Tool | Purpose |
|------|---------|
| `browser_navigate` | Navigate to URL |
| `browser_snapshot` | Get page accessibility tree (for finding elements) |
| `browser_click` | Click an element by reference |
| `browser_type` | Type text into an element |
| `browser_scroll` | Scroll page (up/down) |
| `browser_press_key` | Press keyboard key (Escape, Enter, etc.) |
| `browser_screenshot` | Take screenshot (for debugging) |
| `browser_wait` | Wait for time or element |

## Example Usage

### Incremental crawl (default - recommended for repeat crawls):
```
/crawl_instagram techguru --max-users 5 --output ./crawl_data/ --username myemail@example.com --password mypassword123
```
Output: "Found 3 new posts, skipped 9 existing posts"

### Full re-crawl (ignore existing data):
```
/crawl_instagram techguru --max-users 5 --output ./crawl_data/ --username myemail@example.com --password mypassword123 --full
```
Output: "Full crawl mode - re-crawling all posts"

## Rate Limiting & Safety

- Add 2-5 second delays between actions using `browser_wait`
- If rate limited, pause and retry with exponential backoff
- Respect Instagram's terms of service

## Error Handling

- **Private profiles**: Mark as "private" in summary, skip extraction
- **Rate limits**: Pause, wait, retry with backoff
- **Login walls**: Require `--username` and `--password` for authenticated access
- **2FA required**: Pause and inform user
- **Network errors**: Retry up to 3 times
- **Corrupted existing data**: Fall back to full crawl with warning

## Output Schema

Each user's `data.json`:

```json
{
  "username": "string",
  "crawled_at": "ISO8601 timestamp",
  "first_crawled_at": "ISO8601 timestamp",
  "last_incremental_crawl": "ISO8601 timestamp",
  "crawl_count": "number",
  "profile": {
    "display_name": "string",
    "bio": "string",
    "follower_count": "number",
    "following_count": "number",
    "post_count": "number",
    "is_private": "boolean",
    "is_verified": "boolean"
  },
  "posts": [
    {
      "url": "string",
      "post_id": "string (extracted from URL)",
      "type": "image|video|carousel",
      "caption": "string",
      "like_count": "number",
      "comment_count": "number",
      "timestamp": "ISO8601",
      "crawled_at": "ISO8601 (when this post was first crawled)",
      "likers": [
        {
          "username": "string",
          "display_name": "string"
        }
      ],
      "comments": [
        {
          "username": "string",
          "text": "string",
          "timestamp": "ISO8601",
          "like_count": "number"
        }
      ]
    }
  ]
}
```

## Post Identification

Posts are identified by their URL pattern:
- Reel: `https://www.instagram.com/{username}/reel/{post_id}/`
- Photo/Carousel: `https://www.instagram.com/{username}/p/{post_id}/`

The `post_id` is extracted from the URL and used as a unique identifier to detect duplicates across crawls.

---

## ìµœëŒ€ ë°ì´í„° ìˆ˜ì§‘ ê°€ì´ë“œ (Maximum Data Collection Guide)

### ğŸ¯ ìµœëŒ€í•œ ë§ì€ post, comment, like ìˆ˜ì§‘í•˜ê¸°

#### 1. ê¸°ë³¸ ê¶Œì¥ ì‹¤í–‰ ë°©ë²•
```bash
# ë¡œê·¸ì¸ í•„ìˆ˜ (ë” ë§ì€ ë°ì´í„° ì ‘ê·¼ ê°€ëŠ¥)
/crawl_instagram <seed_username> --max-users 20 --output ./data/ --username <your_ig_email> --password <your_ig_password>
```

#### 2. ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ëŠ˜ë¦¬ëŠ” ì˜µì…˜ë“¤

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--max-users` | í¬ë¡¤ë§í•  ì´ ì‚¬ìš©ì ìˆ˜ | 10 (20-50 ê¶Œì¥, ë„ˆë¬´ ë†’ìœ¼ë©´ rate limit) |
| `--max-comments` | í¬ìŠ¤íŠ¸ë‹¹ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ | 0 (ë¬´ì œí•œ - ëª¨ë“  ëŒ“ê¸€ ì¶”ì¶œ) |
| `--max-likers` | í¬ìŠ¤íŠ¸ë‹¹ ìµœëŒ€ ì¢‹ì•„ìš” ëˆ„ë¥¸ ì‚¬ëŒ ìˆ˜ | 0 (ë¬´ì œí•œ - ëª¨ë“  likers ì¶”ì¶œ) |
| `--full` | ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ì „ì²´ ì¬ìˆ˜ì§‘ | incrementalì´ ê¸°ë³¸ |

#### 3. ë°˜ë³µ ì‹¤í–‰ ì „ëµ (Incremental Crawling)

**ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•: ì£¼ê¸°ì ìœ¼ë¡œ incremental í¬ë¡¤ë§ ë°˜ë³µ**

```bash
# 1ì°¨: ì´ˆê¸° ì „ì²´ ìˆ˜ì§‘
/crawl_instagram creator123 --max-users 30 --output ./data/ --username my@email.com --password pass123

# 2ì°¨ (1-2ì¼ í›„): ìƒˆ í¬ìŠ¤íŠ¸ë§Œ ì¶”ê°€ ìˆ˜ì§‘
/crawl_instagram creator123 --max-users 30 --output ./data/ --username my@email.com --password pass123

# 3ì°¨ (1-2ì¼ í›„): ë˜ ìƒˆ í¬ìŠ¤íŠ¸ ì¶”ê°€...
# ê³„ì† ë°˜ë³µí•˜ë©´ ë°ì´í„°ê°€ ëˆ„ì ë¨
```

#### 4. ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„° í•­ëª©

**ê° í¬ìŠ¤íŠ¸ë§ˆë‹¤ ë°˜ë“œì‹œ ìˆ˜ì§‘:**
- âœ… `url`, `post_id`, `type` (reel/photo/carousel)
- âœ… `caption` (í¬ìŠ¤íŠ¸ ë‚´ìš©)
- âœ… `like_count`, `comment_count`
- âœ… `timestamp` (ê²Œì‹œ ì‹œê°„)
- âœ… `comments[]` - ëª¨ë“  ëŒ“ê¸€ (username, text, timestamp, like_count) - `--max-comments`ë¡œ ì œí•œ ê°€ëŠ¥
- âœ… `likers[]` - ëª¨ë“  ì¢‹ì•„ìš” ëˆ„ë¥¸ ì‚¬ëŒ (username, display_name) - `--max-likers`ë¡œ ì œí•œ ê°€ëŠ¥

#### 5. ì£¼ì˜ì‚¬í•­

| ì´ìŠˆ | í•´ê²°ì±… |
|------|--------|
| Rate limit (ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­) | ìë™ìœ¼ë¡œ 2-5ì´ˆ ë”œë ˆì´ ì ìš©ë¨ |
| ë¹„ê³µê°œ ê³„ì • | ë¡œê·¸ì¸ + íŒ”ë¡œìš° í•„ìš” |
| 2FA ì¸ì¦ ìš”ì²­ | ìˆ˜ë™ìœ¼ë¡œ ì¸ì¦ í›„ ì¬ì‹œë„ |
| ë¡œê·¸ì¸ ì„¸ì…˜ ë§Œë£Œ | ë¸Œë¼ìš°ì €ë¥¼ ë‹«ê³  ë‹¤ì‹œ ì‹œì‘ |

#### 6. ëŒ€ëŸ‰ ìˆ˜ì§‘ ì˜ˆì‹œ

```bash
# ğŸš€ ìµœëŒ€ ìˆ˜ì§‘: ëª¨ë“  comments, likers ì¶”ì¶œ (ê¸°ë³¸ê°’)
/crawl_instagram popular_influencer --max-users 30 --output ./data/ --username your@email.com --password yourpassword --full

# âš¡ ë¹ ë¥¸ ìˆ˜ì§‘: comments 50ê°œ, likers 100ëª…ìœ¼ë¡œ ì œí•œ
/crawl_instagram popular_influencer --max-users 30 --output ./data/ --username your@email.com --password yourpassword --max-comments 50 --max-likers 100

# ğŸ“ˆ ì´í›„ ë§¤ì¼ incremental (ìƒˆ í¬ìŠ¤íŠ¸ë§Œ)
/crawl_instagram popular_influencer --max-users 30 --output ./data/ --username your@email.com --password yourpassword
```

#### 7. ê²°ê³¼ í™•ì¸

```bash
# ìˆ˜ì§‘ëœ ì „ì²´ ìš”ì•½ ë³´ê¸°
cat ./data/crawl_summary.json

# íŠ¹ì • ìœ ì € ë°ì´í„° í™•ì¸
cat ./data/<username>/data.json
```

**Tip:** í•œ ë²ˆì— ë„ˆë¬´ ë§ì€ ìœ ì €(50+)ë¥¼ í¬ë¡¤ë§í•˜ë©´ Instagramì—ì„œ ì¼ì‹œ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 20-30ëª…ì”© ë‚˜ëˆ ì„œ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
